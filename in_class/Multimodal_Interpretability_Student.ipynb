{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75Mfb9Sr6B4-"
      },
      "source": [
        "# Multimodal Interpretability\n",
        "\n",
        "In this notebook, we are going to illustrate how to fine-tune the Vision-and-Language Transformer (ViLT) for visual question answering. This is going to be very similar to how one would fine-tune BERT: one just places a head on top that is randomly initialized, and trains it end-to-end together with a pre-trained base.  Subsequently, we will explore application of the Layer Integrated Gradients method.\n",
        "\n",
        "* Paper: https://arxiv.org/abs/2102.03334\n",
        "* ViLT docs: https://huggingface.co/docs/transformers/master/en/model_doc/vilt\n",
        "\n",
        "We will ask you to answer a couple short-form questions and complete a code block.\n",
        "\n",
        "## Set-up environment\n",
        "\n",
        "First, we install the Transformers library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3CEYz9215-vH"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAicQ-VYbtTa"
      },
      "source": [
        "Next, we mount Google Drive if using the Colab environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "c1rr5xJ5b0ny",
        "outputId": "54eb311d-1566-492f-f480-c4506f76d07a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://images.cocodataset.org/zips/val2014.zip\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDn7rDTZCKRw",
        "outputId": "ddd3add6-954d-4f33-bd84-d6bc0d151da0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-02 21:02:47--  http://images.cocodataset.org/zips/val2014.zip\n",
            "Resolving images.cocodataset.org (images.cocodataset.org)... 52.217.46.244, 54.231.165.9, 16.182.70.153, ...\n",
            "Connecting to images.cocodataset.org (images.cocodataset.org)|52.217.46.244|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6645013297 (6.2G) [application/zip]\n",
            "Saving to: ‘val2014.zip’\n",
            "\n",
            "val2014.zip         100%[===================>]   6.19G  15.6MB/s    in 7m 41s  \n",
            "\n",
            "2025-10-02 21:10:29 (13.7 MB/s) - ‘val2014.zip’ saved [6645013297/6645013297]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1fSEBPw_CR2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "boe1Duge6Jsa"
      },
      "source": [
        "## Load data\n",
        "\n",
        "Next, we load the data. The data of VQAv2 can be obtained from the [official website](https://visualqa.org/download.html).\n",
        "\n",
        "For demonstration purposes, we only download the validation dataset. We download:\n",
        "* the images (stored in a single folder)\n",
        "* the questions (stored in a JSON)\n",
        "* the annotations (stored in a JSON) a.k.a. the answers to the questions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgTMR9H5kkHw"
      },
      "source": [
        "### Read questions\n",
        "\n",
        "First, we read the questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "XKAWRzGk7mPe",
        "outputId": "85d7985d-a9aa-44ba-8cec-0b432000cea3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/ViLT/Datasets/VQAv2/v2_OpenEnded_mscoco_val2014_questions.json'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4094687645.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Opening JSON file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/ViLT/Datasets/VQAv2/v2_OpenEnded_mscoco_val2014_questions.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Return JSON object as dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/ViLT/Datasets/VQAv2/v2_OpenEnded_mscoco_val2014_questions.json'"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "# Opening JSON file\n",
        "f = open('/content/drive/MyDrive/ViLT/Datasets/VQAv2/v2_OpenEnded_mscoco_val2014_questions.json')\n",
        "\n",
        "# Return JSON object as dictionary\n",
        "data_questions = json.load(f)\n",
        "print(data_questions.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sqaHntDmx_V"
      },
      "source": [
        "Let's see how many questions there are:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v48dQa0umz32"
      },
      "outputs": [],
      "source": [
        "questions = data_questions['questions']\n",
        "print(\"Number of questions:\", len(questions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uigy071flj4h"
      },
      "source": [
        "That's quite a lot! Let's take a look at the first one:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xKiXtLz75co"
      },
      "outputs": [],
      "source": [
        "questions[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDWUI551l0Ed"
      },
      "source": [
        "As we can see, this question is related to an image with a certain ID. How can we find back which image this is? The function below allows to get the ID from a corresponding filename. We'll use it to map between image IDs and their corresponding filenames."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OrkfJmHE77l4"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from typing import Optional\n",
        "\n",
        "filename_re = re.compile(r\".*(\\d{12})\\.((jpg)|(png))\")\n",
        "\n",
        "def id_from_filename(filename: str) -> Optional[int]:\n",
        "    match = filename_re.fullmatch(filename)\n",
        "    if match is None:\n",
        "        return None\n",
        "    return int(match.group(1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxBw6v-V9Zx0"
      },
      "outputs": [],
      "source": [
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# root at which all images are stored\n",
        "root = '/content/drive/MyDrive/ViLT/Datasets/VQAv2/val2014'\n",
        "file_names = [f for f in tqdm(listdir(root)) if isfile(join(root, f))]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJc7lj_omJiS"
      },
      "source": [
        "We can map a filename to its ID as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NxIWvGF59rNz"
      },
      "outputs": [],
      "source": [
        "id_from_filename('COCO_val2014_000000501080.jpg')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PP0Uww0bmMcI"
      },
      "source": [
        "We create 2 dictionaries, one that maps filenames to their IDs and one the other way around:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "375ue4AT9r8J"
      },
      "outputs": [],
      "source": [
        "filename_to_id = {root + \"/\" + file: id_from_filename(file) for file in file_names}\n",
        "id_to_filename = {v:k for k,v in filename_to_id.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYbTBfN_mT9B"
      },
      "source": [
        "We can now find back the image to which the question 'Where is he looking?' corresponded:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZeW4M0yU-boe"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "print(f\"Size of map: {len(id_to_filename)}\")\n",
        "\n",
        "path = id_to_filename[questions[0]['image_id']]\n",
        "image = Image.open(path)\n",
        "image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jfx7Bw_0ltc0"
      },
      "source": [
        "### Read annotations\n",
        "\n",
        "Next, let's read the annotations. As we'll see, every image is annotated with multiple possible answers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1iiGdFMy6Blc"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Read annotations\n",
        "f = open('/content/drive/MyDrive/ViLT/Datasets/VQAv2/v2_mscoco_val2014_annotations.json')\n",
        "\n",
        "# Return JSON object as dictionary\n",
        "data_annotations = json.load(f)\n",
        "print(data_annotations.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8XE6I9vkYA8"
      },
      "source": [
        "As we can see, there are 214354 annotations in total (for the validation dataset only!)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IL5ohlCADj8X"
      },
      "outputs": [],
      "source": [
        "annotations = data_annotations['annotations']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UxA_dg3C7fRh"
      },
      "outputs": [],
      "source": [
        "print(\"Number of annotations:\", len(annotations))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ar2eN6NGkcFK"
      },
      "source": [
        "Let's take a look at the first one. As we can see, the example contains several answers (collected by different human annotators). The answer to a question can be a bit subjective: for instance for the question \"where is he looking?\", some people annotated this with \"down\", others with \"table\", another one with \"skateboard\", etc. So there's a bit of disambiguity among the annotators :)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KpmOLyBj7kyg"
      },
      "outputs": [],
      "source": [
        "annotations[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NX43IjmFedOc"
      },
      "source": [
        "## Add labels + scores\n",
        "\n",
        "Due to this ambiguity, most authors treat the VQAv2 dataset as a multi-label classification problem (as multiple answers are possibly valid). Moreover, rather than just creating a one-hot encoded vector, one creates a soft encoding, based on the number of times a certain answer appeared in the annotations.\n",
        "\n",
        "For instance, in the example above, the answer \"down\" seems to be selected way more often than \"skateboard\". Hence, we want the model to give more emphasis on \"down\" than on \"skateboard\". We can achieve this by giving a score of 1.0 to labels which are counted at least 3 times, and a score < 1.0 for labels that are counted less.\n",
        "\n",
        "We'll add 2 keys to each annotations:\n",
        "* labels, which is a list of integer indices of the labels that apply to a given image + question.\n",
        "* scores, which are the corresponding scores (between 0 and 1), which indicate the importance of each label.\n",
        "\n",
        "As we'll need the id2label mapping from the VQA dataset, we load it from the hub as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQK-4nIa_6SX"
      },
      "outputs": [],
      "source": [
        "from transformers import ViltConfig\n",
        "\n",
        "config = ViltConfig.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XdPv_6rt-soT"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "\n",
        "def get_score(count: int) -> float:\n",
        "    return min(1.0, count / 3)\n",
        "\n",
        "for annotation in tqdm(annotations):\n",
        "    answers = annotation['answers']\n",
        "    answer_count = {}\n",
        "    for answer in answers:\n",
        "        answer_ = answer[\"answer\"]\n",
        "        answer_count[answer_] = answer_count.get(answer_, 0) + 1\n",
        "    labels = []\n",
        "    scores = []\n",
        "    for answer in answer_count:\n",
        "        if answer not in list(config.label2id.keys()):\n",
        "            continue\n",
        "        labels.append(config.label2id[answer])\n",
        "        score = get_score(answer_count[answer])\n",
        "        scores.append(score)\n",
        "    annotation['labels'] = labels\n",
        "    annotation['scores'] = scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1ARWJ9bekb6"
      },
      "source": [
        "Let's verify an example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAvbba0vEW8O"
      },
      "outputs": [],
      "source": [
        "annotations[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "japvxlFjvV1N"
      },
      "source": [
        "Let's verify the labels and corresponding scores:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nYBAwEr9BX0v"
      },
      "outputs": [],
      "source": [
        "labels = annotations[0]['labels']\n",
        "print([config.id2label[label] for label in labels])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DW0uAJ96AyAR"
      },
      "outputs": [],
      "source": [
        "scores = annotations[0]['scores']\n",
        "print(scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0A7kE6HYGET9"
      },
      "source": [
        "## Create PyTorch dataset\n",
        "\n",
        "Next, we create a regular [PyTorch dataset](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html). We leverage `ViltProcessor` to prepare each image + text pair for the model, which will automatically:\n",
        "* leverage `BertTokenizerFast` to tokenize the text and create `input_ids`, `attention_mask` and `token_type_ids`\n",
        "* leverage `ViltImageProcessor` to resize + normalize the image and create `pixel_values` and `pixel_mask`.\n",
        "\n",
        "Note that the docs of `ViltProcessor` can be found [here](https://huggingface.co/docs/transformers/master/en/model_doc/vilt#transformers.ViltProcessor).\n",
        "\n",
        "We also add the labels. This is a PyTorch tensor of shape `(num_labels,)` that contains the soft encoded vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJ3sm7qGAyab"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "class VQADataset(torch.utils.data.Dataset):\n",
        "    \"\"\"VQA (v2) dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, questions, annotations, processor):\n",
        "        self.questions = questions\n",
        "        self.annotations = annotations\n",
        "        self.processor = processor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.annotations)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # get image + text\n",
        "        annotation = self.annotations[idx]\n",
        "        questions = self.questions[idx]\n",
        "        image = Image.open(id_to_filename[annotation['image_id']])\n",
        "        text = questions['question']\n",
        "\n",
        "        encoding = self.processor(image, text, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "        # remove batch dimension\n",
        "        for k,v in encoding.items():\n",
        "          encoding[k] = v.squeeze()\n",
        "        # add labels\n",
        "        labels = annotation['labels']\n",
        "        scores = annotation['scores']\n",
        "        targets = torch.zeros(len(config.id2label))\n",
        "        for label, score in zip(labels, scores):\n",
        "              targets[label] = score\n",
        "        encoding[\"labels\"] = targets\n",
        "\n",
        "        return encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dl2UsPrTHbtu"
      },
      "outputs": [],
      "source": [
        "from transformers import ViltProcessor\n",
        "\n",
        "processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-mlm\")\n",
        "\n",
        "dataset = VQADataset(questions=questions[:100],\n",
        "                     annotations=annotations[:100],\n",
        "                     processor=processor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKFd8ozdIY_q"
      },
      "outputs": [],
      "source": [
        "dataset[0].keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FBcmLQ6AJb4M"
      },
      "outputs": [],
      "source": [
        "processor.decode(dataset[0]['input_ids'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGxYDVSve-O6"
      },
      "outputs": [],
      "source": [
        "labels = torch.nonzero(dataset[0]['labels']).squeeze().tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__LNlpeffBh-"
      },
      "outputs": [],
      "source": [
        "[config.id2label[label] for label in labels]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tThbn-ilfmYi"
      },
      "source": [
        "## Define model\n",
        "\n",
        "Here we define a `ViltForQuestionAnswering` model, with the weights of the body initialized from dandelin/vilt-b32-mlm, and a randomly initialized classification head. We also move it to the GPU, if it's available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sjOH0jYcfkAk"
      },
      "outputs": [],
      "source": [
        "from transformers import ViltForQuestionAnswering\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = ViltForQuestionAnswering.from_pretrained(\"dandelin/vilt-b32-mlm\",\n",
        "                                                 id2label=config.id2label,\n",
        "                                                 label2id=config.label2id)\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8KRHccEw5DE"
      },
      "source": [
        "Next, we create a corresponding PyTorch DataLoader, which allows us to iterate over the dataset in batches.\n",
        "\n",
        "Due to the fact that the processor resizes images to not necessarily the same size, we leverage the `pad_and_create_pixel_mask` method of the processor to pad the pixel values of a batch and create a corresponding pixel mask, which is a tensor of shape (batch_size, height, width) indicating which pixels are real (1) and which are padding (0)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9C4gCxrbgjqh"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def collate_fn(batch):\n",
        "  input_ids = [item['input_ids'] for item in batch]\n",
        "  pixel_values = [item['pixel_values'] for item in batch]\n",
        "  attention_mask = [item['attention_mask'] for item in batch]\n",
        "  token_type_ids = [item['token_type_ids'] for item in batch]\n",
        "  labels = [item['labels'] for item in batch]\n",
        "\n",
        "  # create padded pixel values and corresponding pixel mask\n",
        "  encoding = processor.image_processor.pad(pixel_values, return_tensors=\"pt\")\n",
        "\n",
        "  # create new batch\n",
        "  batch = {}\n",
        "  batch['input_ids'] = torch.stack(input_ids)\n",
        "  batch['attention_mask'] = torch.stack(attention_mask)\n",
        "  batch['token_type_ids'] = torch.stack(token_type_ids)\n",
        "  batch['pixel_values'] = encoding['pixel_values']\n",
        "  batch['pixel_mask'] = encoding['pixel_mask']\n",
        "  batch['labels'] = torch.stack(labels)\n",
        "\n",
        "  return batch\n",
        "\n",
        "train_dataloader = DataLoader(dataset, collate_fn=collate_fn, batch_size=4, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question**: What is the purpose of the collation function?\n",
        "\n",
        "**Answer**: *TODO*"
      ],
      "metadata": {
        "id": "XljebTOC0Kg-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvOs133CxoP-"
      },
      "source": [
        "Let's verify a batch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yKBOBxq1gqzy"
      },
      "outputs": [],
      "source": [
        "batch = next(iter(train_dataloader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lZkar-taiq1r"
      },
      "outputs": [],
      "source": [
        "for k,v in batch.items():\n",
        "  print(k, v.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ssFPFDKBzWT"
      },
      "source": [
        "We can verify a given training example, by visualizing the image:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4WI05xuYB1_3"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "image_mean = processor.image_processor.image_mean\n",
        "image_std = processor.image_processor.image_std\n",
        "\n",
        "batch_idx = 1\n",
        "\n",
        "unnormalized_image = (batch[\"pixel_values\"][batch_idx].numpy() * np.array(image_mean)[:, None, None]) + np.array(image_std)[:, None, None]\n",
        "unnormalized_image = (unnormalized_image * 255).astype(np.uint8)\n",
        "unnormalized_image = np.moveaxis(unnormalized_image, 0, -1)\n",
        "Image.fromarray(unnormalized_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jde185szCRSk"
      },
      "outputs": [],
      "source": [
        "processor.decode(batch[\"input_ids\"][batch_idx])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g1eAPG32CduY"
      },
      "outputs": [],
      "source": [
        "labels = torch.nonzero(batch['labels'][batch_idx]).squeeze().tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CB7SOl09CduZ"
      },
      "outputs": [],
      "source": [
        "[config.id2label[label] for label in labels]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uN8Grgg0gK2x"
      },
      "source": [
        "## Train a model\n",
        "\n",
        "Finally, let's train a model! Note that I haven't done any hyperparameter tuning as this notebook was just created for demo purposes. I'd recommend going over the [ViLT paper](https://arxiv.org/abs/2102.03334) for better training settings. You may also wish to use PyTorch Lightning for real training setups.\n",
        "\n",
        "I just wanted to illustrate that you can make the model overfit this dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57PlqyxXf6L4"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "model.train()\n",
        "for epoch in range(25):  # loop over the dataset multiple times\n",
        "   print(f\"Epoch: {epoch}\")\n",
        "   for batch in tqdm(train_dataloader):\n",
        "        # get the inputs;\n",
        "        batch = {k:v.to(device) for k,v in batch.items()}\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        print(\"Loss:\", loss.item())\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aj5anZ-ZyMEY"
      },
      "source": [
        "## Inference\n",
        "\n",
        "Let's verify whether the model has actually learned something:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qG0UzoCfgpEF"
      },
      "outputs": [],
      "source": [
        "example = dataset[0]\n",
        "print(example.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDIsLxBMAdRI"
      },
      "outputs": [],
      "source": [
        "processor.decode(example['input_ids'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eszu33n7yS4f"
      },
      "outputs": [],
      "source": [
        "# add batch dimension + move to GPU\n",
        "example = {k: v.unsqueeze(0).to(device) for k,v in example.items()}\n",
        "\n",
        "# forward pass\n",
        "outputs = model(**example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxSx-roGDh1a"
      },
      "source": [
        "Note that we need to apply a sigmoid activation on the logits since the model is trained using binary cross-entropy loss (as it frames VQA as a multi-label classification task)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZ4_ceydyePt"
      },
      "outputs": [],
      "source": [
        "logits = outputs.logits\n",
        "predicted_classes = torch.sigmoid(logits)\n",
        "\n",
        "probs, classes = torch.topk(predicted_classes, 5)\n",
        "probs\n",
        "for prob, class_idx in zip(probs.squeeze().tolist(), classes.squeeze().tolist()):\n",
        "  print(prob, model.config.id2label[class_idx])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, the model requires more work to be effective. This is expected since the training data was not even used.\n",
        "\n",
        "However, this initial checkpoint is sufficient for validating the end-to-end pipeline."
      ],
      "metadata": {
        "id": "u1Hzeaup6sYl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interpretability\n",
        "This section is adapted from the following tutorial: https://captum.ai/tutorials/Multimodal_VQA_Interpret"
      ],
      "metadata": {
        "id": "nnWUKzgObmE5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install captum"
      ],
      "metadata": {
        "id": "ZnR6DkxQuf1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy, os, sys\n",
        "\n",
        "# Replace <PROJECT-DIR> placeholder with your project directory path\n",
        "PROJECT_DIR = '/content'"
      ],
      "metadata": {
        "id": "W86M2qXMwSTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOOVAOc5yvbT"
      },
      "outputs": [],
      "source": [
        "import threading\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "\n",
        "from captum.attr import (\n",
        "    IntegratedGradients,\n",
        "    LayerIntegratedGradients,\n",
        "    TokenReferenceBase,\n",
        "    configure_interpretable_embedding_layer,\n",
        "    remove_interpretable_embedding_layer,\n",
        "    visualization\n",
        ")\n",
        "from captum.attr._utils.input_layer_wrapper import ModelInputWrapper"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()"
      ],
      "metadata": {
        "id": "cYMBQ7SVun4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download example images [here](https://github.com/pytorch/captum/blob/master/tutorials/img/vqa/), place them in `img/vqa` directory."
      ],
      "metadata": {
        "id": "GtO6bP8JwF1h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to explain text features, we must let integrated gradients attribute on the embeddings, not the indices. The reason for this is simply due to Integrated Gradients being a gradient-based attribution method, as we are unable to compute gradients with respect to integers.\n",
        "\n",
        "Hence, we have two options:\n",
        "1. \"Patch\" the model's embedding layer and corresponding inputs. To patch the layer, use the `configure_interpretable_embedding_layer`^ method, which will wrap the associated layer you give it, with an identity function. This identity function accepts an embedding and outputs an embedding. You can patch the inputs, i.e. obtain the embedding for a set of indices, with `model.wrapped_layer.indices_to_embeddings(indices)`.\n",
        "2. Use the equivalent layer attribution algorithm (`LayerIntegratedGradients` in our case) with the utility class `ModelInputWrapper`. The `ModelInputWrapper` will wrap your model and feed all it's inputs to seperate layers; allowing you to use layer attribution methods on inputs. You can access the associated layer for input named `\"foo\"` via the `ModuleDict`: `wrapped_model.input_maps[\"foo\"]`.\n",
        "\n",
        "^ NOTE: For option (1), after finishing interpretation it is important to call `remove_interpretable_embedding_layer` which removes the Interpretable Embedding Layer that we added for interpretation purposes and sets the original embedding layer back in the model.\n",
        "\n",
        "It is recommended to do option (2) since this option is much more flexible and easy to use. The reason it is more flexible is it allows your model to do any sort of preprocessing to the indices tensor. It's easier to use since you don't have to touch your inputs."
      ],
      "metadata": {
        "id": "kc2jrDaDwftD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "See documentation on `LayerIntegratedGradients`:\n",
        "https://captum.ai/api/layer.html#layer-integrated-gradients"
      ],
      "metadata": {
        "id": "0zZfDx5rxl1N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This wrapper is useful for dealing with the dictionary arguments."
      ],
      "metadata": {
        "id": "RAA5AHgc8PQr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ViltModelWrapper(torch.nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "\n",
        "    def forward(self, input_ids, pixel_values, attention_mask=None, pixel_mask=None, token_type_ids=None):\n",
        "        # Construct the dictionary expected by the original model's forward method\n",
        "        inputs = {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"pixel_values\": pixel_values,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"pixel_mask\": pixel_mask,\n",
        "            \"token_type_ids\": token_type_ids,\n",
        "        }\n",
        "        # Filter out None values\n",
        "        inputs = {k: v for k, v in inputs.items() if v is not None}\n",
        "        outputs = self.model(**inputs)\n",
        "        # Return logits directly\n",
        "        return outputs.logits\n",
        "\n",
        "# model wrapper to handle dictionary args\n",
        "wrap_model = ViltModelWrapper(model)\n",
        "\n",
        "# wrap the inputs into layers for attribution\n",
        "m = ModelInputWrapper(wrap_model)"
      ],
      "metadata": {
        "id": "yvP9dw7VxmLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implementation**:\n",
        "\n",
        "Here you should create a `LayerIntegratedGradients` object using the wrapped module `m`.\n",
        "\n",
        "Try reading the documentation, the linked tutorial, and inspecting the wrapped module `m` for help."
      ],
      "metadata": {
        "id": "hfz7WxZT8TaC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: create layer integrated gradients object\n",
        "# attr = LayerIntegratedGradients(...)"
      ],
      "metadata": {
        "id": "mbOJowQDxEiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining default cmap that will be used for image visualizations"
      ],
      "metadata": {
        "id": "GgP38xUUwhLP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "default_cmap = LinearSegmentedColormap.from_list('custom blue',\n",
        "                                                 [(0, '#ffffff'),\n",
        "                                                  (0.25, '#252b36'),\n",
        "                                                  (1, '#000000')], N=256)"
      ],
      "metadata": {
        "id": "2Xum8QEfyXcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining a few test images for model intepretation purposes"
      ],
      "metadata": {
        "id": "S-eatXHJyZcx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "images = ['/content/drive/MyDrive/img/vqa/siamese.jpg',\n",
        "          '/content/drive/MyDrive/img/vqa/elephant.jpg',\n",
        "          '/content/drive/MyDrive/img/vqa/zebra.jpg']"
      ],
      "metadata": {
        "id": "dFTvUM6Pyb3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vilt_interpret(image_filename, questions, targets):\n",
        "    img = Image.open(image_filename)\n",
        "\n",
        "    for question, target in zip(questions, targets):\n",
        "        inputs = processor(img, question, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "        additional_args = (\n",
        "            inputs.get('attention_mask').to(device),\n",
        "            inputs.get('pixel_mask').to(device),\n",
        "            inputs.get('token_type_ids').to(device)\n",
        "        )\n",
        "\n",
        "        image_baseline = torch.zeros_like(inputs[\"pixel_values\"])\n",
        "        text_baseline = torch.full_like(inputs[\"input_ids\"], processor.tokenizer.pad_token_id)\n",
        "\n",
        "        logits = m(**inputs)\n",
        "        predictions = torch.sigmoid(logits)\n",
        "        probs, classes = predictions.topk(1, dim=1)\n",
        "        predicted_class = classes.item()\n",
        "        predicted_prob = probs.item()\n",
        "        true_class = config.label2id[target]\n",
        "\n",
        "        attributions = attr.attribute(inputs=(inputs[\"input_ids\"], inputs[\"pixel_values\"]),\n",
        "                                      baselines=(text_baseline, image_baseline),\n",
        "                                      target=true_class,   # Use true class as target\n",
        "                                      additional_forward_args=additional_args,\n",
        "                                      n_steps=30)\n",
        "\n",
        "        # Normalize text attributions.\n",
        "        text_attributions = attributions[1].sum(dim=2).squeeze(0)\n",
        "        text_attributions_norm = text_attributions / text_attributions.norm()\n",
        "        tokens = processor.tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
        "\n",
        "        # Visualize text attributions\n",
        "        vis_data_record = visualization.VisualizationDataRecord(\n",
        "            word_attributions=text_attributions_norm.tolist(),\n",
        "            pred_prob=predicted_prob,\n",
        "            pred_class=config.id2label[predicted_class],\n",
        "            true_class=config.id2label[true_class],  # Adjust if you have the true class available\n",
        "            attr_class=target,  # The class for which attributions were computed\n",
        "            attr_score=attributions[1].sum().item(),\n",
        "            raw_input_ids=tokens,\n",
        "            convergence_score=0.0\n",
        "        )\n",
        "        visualization.visualize_text([vis_data_record])\n",
        "\n",
        "        # Visualize image attributions\n",
        "        original_im_mat = np.transpose(inputs['pixel_values'].squeeze(0).cpu().detach().numpy(), (1, 2, 0))\n",
        "\n",
        "        # Get mean and std for image processor\n",
        "        image_mean = np.array(processor.image_processor.image_mean)\n",
        "        image_std = np.array(processor.image_processor.image_std)\n",
        "\n",
        "        # Denormalize using the provided mean and std\n",
        "        original_im_mat = (original_im_mat * image_std) + image_mean  # Apply denormalization\n",
        "        original_im_mat = np.clip(original_im_mat, 0, 1)  # Ensure values are in [0, 1] range\n",
        "        original_im_mat = (original_im_mat * 255).astype(np.uint8)  # Scale to [0, 255] range and convert to integers\n",
        "\n",
        "        # Reshape attributions tensor\n",
        "        attributions_img = np.transpose(attributions[0].squeeze(0).cpu().detach().numpy(), (1, 2, 0))\n",
        "\n",
        "        visualization.visualize_image_attr_multiple(attributions_img, original_im_mat,\n",
        "                                                    [\"original_image\", \"heat_map\"], [\"all\", \"absolute_value\"],\n",
        "                                                    titles=[\"Original Image\", \"Attribution Magnitude\"],\n",
        "                                                    cmap=default_cmap,\n",
        "                                                    show_colorbar=True)\n",
        "\n",
        "        print('Image Contributions: ', attributions[0].sum().item())\n",
        "        print('Text Contributions: ', attributions[1].sum().item())\n",
        "        print('Total Contribution: ', attributions[0].sum().item() + attributions[1].sum().item())"
      ],
      "metadata": {
        "id": "WwriS5R8ysxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question**: What is the purpose of the baseline inputs?\n",
        "\n",
        "**Answer**: *TODO*"
      ],
      "metadata": {
        "id": "5sk5lOb90Cf5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now generate visualizations for each of the example images."
      ],
      "metadata": {
        "id": "MUUStHlO-HnB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the index of image in the test set. Please, change it if you want to play with different test images/samples.\n",
        "image_idx = 1 # elephant\n",
        "vilt_interpret(images[image_idx], [\n",
        "    \"what is on the picture\",\n",
        "    \"what color is the elephant\",\n",
        "    \"where is the elephant\"\n",
        "], ['elephant', 'gray', 'zoo'])"
      ],
      "metadata": {
        "id": "WcEXoBKG0uZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_idx = 0 # cat\n",
        "\n",
        "vilt_interpret(images[image_idx], [\n",
        "    \"what is on the picture\",\n",
        "    \"what color are the cat's eyes\",\n",
        "    \"is the animal in the picture a cat or a fox\",\n",
        "    \"what color is the cat\",\n",
        "    \"how many ears does the cat have\",\n",
        "], ['cat', 'blue', 'cat', 'white and brown', '2'])"
      ],
      "metadata": {
        "id": "i5SQUTYH13Nu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_idx = 2 # zebra\n",
        "\n",
        "vilt_interpret(images[image_idx], [\n",
        "    \"what is on the picture\",\n",
        "    \"what color are the zebras\",\n",
        "    \"how many zebras are on the picture\",\n",
        "    \"where are the zebras\"\n",
        "], ['zebra', 'black and white', '2', 'zoo'])"
      ],
      "metadata": {
        "id": "nYF_HKL52QyU"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}